{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p5e4F_xxiqps"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Maze setup: 0 -> Free space, 1 -> Wall, 2 -> Goal\n",
        "maze = np.array([\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [0, 1, 0, 1, 0],\n",
        "    [0, 0, 0, 1, 0],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [0, 0, 0, 0, 2]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1   # Learning rate\n",
        "gamma = 0.9   # Discount factor\n",
        "epsilon = 0.2 # Exploration factor\n",
        "episodes = 1000\n",
        "\n",
        "# Define actions\n",
        "actions = ['up', 'down', 'left', 'right']"
      ],
      "metadata": {
        "id": "E9XxDD0ai4Ws"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Q-table with zeros\n",
        "q_table = np.zeros((maze.shape[0], maze.shape[1], len(actions)))\n",
        "\n",
        "# Helper functions\n",
        "def get_next_position(row, col, action):\n",
        "    if action == 'up':\n",
        "        return max(row - 1, 0), col\n",
        "    elif action == 'down':\n",
        "        return min(row + 1, maze.shape[0] - 1), col\n",
        "    elif action == 'left':\n",
        "        return row, max(col - 1, 0)\n",
        "    elif action == 'right':\n",
        "        return row, min(col + 1, maze.shape[1] - 1)\n",
        "\n",
        "def get_reward(next_row, next_col):\n",
        "    if maze[next_row, next_col] == 2:  # Goal state\n",
        "        return 100\n",
        "    elif maze[next_row, next_col] == 1:  # Wall state\n",
        "        return -100\n",
        "    else:\n",
        "        return -1  # Default step reward\n",
        "\n",
        "def is_terminal_state(row, col):\n",
        "    return maze[row, col] == 2\n",
        "\n",
        "# Training\n",
        "for episode in range(episodes):\n",
        "    # Start at random position in the maze\n",
        "    row, col = np.random.randint(maze.shape[0]), np.random.randint(maze.shape[1])\n",
        "\n",
        "    # Continue exploring until the goal is reached\n",
        "    while not is_terminal_state(row, col):\n",
        "        # Choose action (epsilon-greedy)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action_idx = np.random.randint(len(actions))\n",
        "        else:\n",
        "            action_idx = np.argmax(q_table[row, col])\n",
        "\n",
        "        # Take action\n",
        "        next_row, next_col = get_next_position(row, col, actions[action_idx])\n",
        "        reward = get_reward(next_row, next_col)\n",
        "\n",
        "        # Update Q-value\n",
        "        best_next_action = np.argmax(q_table[next_row, next_col])\n",
        "        q_table[row, col, action_idx] = q_table[row, col, action_idx] + \\\n",
        "            alpha * (reward + gamma * q_table[next_row, next_col, best_next_action] - q_table[row, col, action_idx])\n",
        "\n",
        "        # Move to the next state\n",
        "        row, col = next_row, next_col\n",
        "\n",
        "# Print learned Q-table (optimal policy)\n",
        "print(\"\\nLearned Q-table:\")\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "id": "qI7wGTPPjspH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d89dfd2-900e-451e-dde7-8a6067895ad6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learned Q-table:\n",
            "[[[ 13.84880063  41.75258214   8.67707306 -53.96356829]\n",
            "  [-36.17881376 -19.009       -1.13619118  44.48653923]\n",
            "  [ 12.15208261   5.90891542 -37.48966213  54.95021328]\n",
            "  [ 21.51287747 -38.05800601  13.62446824  62.17095131]\n",
            "  [ 33.47125577  70.18999986  24.0792001   44.59931717]]\n",
            "\n",
            " [[ 13.55907429  48.45814784  14.495431   -61.76163484]\n",
            "  [-42.05063479   7.41408031  41.5022142   -0.80607188]\n",
            "  [ 46.74331728   5.44735915 -40.20417662 -29.18061124]\n",
            "  [ 19.47893093 -23.66596488   3.33948319  70.14262825]\n",
            "  [ 54.87520373  79.1        -39.23806164  58.9968373 ]]\n",
            "\n",
            " [[ 34.93952342  54.95389997  31.65569687  18.22309538]\n",
            "  [-38.12886902 -31.43063988  48.45261737   5.00990684]\n",
            "  [ -1.8303419  -33.42738238  41.80923725 -33.78926647]\n",
            "  [-21.69174785 -25.99546715   7.85602674  78.89405143]\n",
            "  [ 64.98535457  89.         -38.9471248   70.87201919]]\n",
            "\n",
            " [[ 34.42619902  62.171       33.75718202 -36.94469715]\n",
            "  [ 20.93625255  70.10973819  19.26088255 -19.93893536]\n",
            "  [  9.56236658  79.00555767 -19.2104792  -12.76106256]\n",
            "  [-21.76885263  88.99524542 -19.54083331  36.38323002]\n",
            "  [ 74.43226272 100.         -22.11362318  83.95646123]]\n",
            "\n",
            " [[ 41.012666    53.27906057  49.69061302  70.19      ]\n",
            "  [-39.33032187  67.03635994  53.58050116  79.1       ]\n",
            "  [-32.42651708  73.46923583  63.01718214  89.        ]\n",
            "  [-20.61321849  87.55020487  76.70326845 100.        ]\n",
            "  [  0.           0.           0.           0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualization of the optimal policy\n",
        "policy = np.chararray(maze.shape, unicode=True)\n",
        "policy[:] = ' '\n",
        "\n"
      ],
      "metadata": {
        "id": "VDWWyNiSp1EM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(maze.shape[0]):\n",
        "    for j in range(maze.shape[1]):\n",
        "        if maze[i, j] == 1:\n",
        "            policy[i, j] = '#'\n",
        "        elif maze[i, j] == 2:\n",
        "            policy[i, j] = 'G'\n",
        "        else:\n",
        "            best_action = np.argmax(q_table[i, j])\n",
        "            policy[i, j] = actions[best_action][0].upper()\n",
        "\n",
        "print(\"\\nLearned Policy:\")\n",
        "print(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpD90KDLvp6-",
        "outputId": "2cdc8e86-44d5-440b-935b-e9097bd5e9d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learned Policy:\n",
            "[['D' '#' 'R' 'R' 'D']\n",
            " ['D' '#' 'U' '#' 'D']\n",
            " ['D' 'L' 'L' '#' 'D']\n",
            " ['D' '#' '#' '#' 'D']\n",
            " ['R' 'R' 'R' 'R' 'G']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1-co_8PwVlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}